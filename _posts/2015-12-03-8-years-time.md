---
layout:     post
title:      8 Years Time
date:       2015-12-203 20:00
summary:    The story behind the 8 year lag in data we saw earlier today
category:   Big Data
tags:       support, hadoop
author:     alice_kaerast
---

![Graph of 8-year spike](/images/8yearspike.png)

Earlier today this graph from our datawarehouse system was tweeted out, and got some amused responses about 2007 wanting its data back.  But there's actually a good story here, and a chance to look at Hadoop's dirty secrets.

Picture if you will a graph showing the time between now and the last record to be inserted into a table.  For a regular batch operation, you'd expect a nice saw tooth graph - slowly ramping up for 20 minutes, followed by a sharp drop to around 5 minutes.  The minimum value is the time it takes for the job to run, the height of the graph is how long between runs.

If you were to zoom into the flat portion of this graph, that's not what you'd see.  It's actually a little more jagged.  To explain this, lets look at the process it's graphing.

Data is sqooped into HDFS from a standard RDBMS.  It's then processed in Hive, and we perform an insert overwrite into the production Hive table.  But this graph is generated using Impala, the interface that our BI tool uses.  In order for Impala to know about changed data in a Hive table, you need to perform a refresh to update the catalog server.  Since the Impala refresh command is out of sync with the imports, we get a jagged sawtooth graph.

But a few times a day you get something worse than that jagged sawtooth graph.  A few times a day the refresh command will run during the Hive import.  This causes bad things to happen.

Whilst the insert overwrite command in Hive is atomic, doing all the work on the files before moving them into place, that file movement can take a few minutes.  So if the Impala refresh command runs during the Hive insert, it will only pick up files that have actually been written - which might be just the very first file, with data from the first few years of the company's history.

And this is the dirty secret.  Whilst you can read HDFS data using Hive, Spark, Impala, Pig, etc. Unless you're using the same tool as the writer, you're bound to hit this sort of problem pretty frequently.

So how do we resolve it?

Well, the first and obvious option is to use a single tool.  Migrate all your pipelines to the same tool that your users read data through - Spark or Impala would be a good choice.  Except Spark is hard for end-users and BI tools to use, and Impala means lots of SQL which is hard to unit test.  Plus people in finance like SQL, whereas data scientists like R and Spark.

The other option is to copy all of your data into a second storage location, using whatever tool the users use.  We're doing this in limited cases, and it comes with other benefits too.  For instance, we can use Impala to once a day update tables containing the current year's transactions - using a storage format and partition design that suits the business (Parquet and yearly or monthly partitions) but which would cause imports to run terribly (textfile or RCfile and daily partitions).  The obvious drawback here is that you now have two copies of all that data, and whilst disk is cheap it still feels wrong to have to do this.

In truth, neither of these options are perfect.  But a combination of the two is where we are planning on heading over the coming months.

Of course there is a final option - only give your users access to data which isn't changing. Ie. yesterday's data.  But isn't the whole point of Hadoop that you can give more real-time access to all this wonderful data?